{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e58ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas \n",
    "%pip install mne\n",
    "%pip install plotly nbformat>=4.2.0\n",
    "%pip install tensorflowimport os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence TF Warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class Config:\n",
    "    # Dataset\n",
    "    DATA_PATH = \"eeg_data/\" # Local path or download location\n",
    "    CHANNELS = ['Oz', 'T7', 'Cz'] # The 3 optimal channels \n",
    "    SFREQ = 160.0\n",
    "    \n",
    "    # Segmentation Parameters (Table 1) [cite: 1231]\n",
    "    T = 160          # Window length (samples) -> 1.0 sec\n",
    "    ETA = 20         # Number of overlapping segments per input image\n",
    "    DELTA_STRIDE = 4 # Stride between segments (delta)\n",
    "    \n",
    "    # The \"Sampling Window\" F is the total duration required to build one input\n",
    "    # F = (eta - 1) * delta + T = (19 * 4) + 160 = 236 samples\n",
    "    F_SAMPLING_WINDOW = 236 \n",
    "    \n",
    "    # Data Augmentation Stride (Big Delta)\n",
    "    AUGMENTATION_STRIDE = 8 # Stride for creating new inputs [cite: 1231]\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE = 64     # [cite: 1231]\n",
    "    EPOCHS = 30         # [cite: 1231]\n",
    "    LR = 0.0001         # [cite: 1231]\n",
    "    DROPOUT = 0.25      # [cite: 1231]\n",
    "    \n",
    "    # Authentication\n",
    "    THRESHOLD = 0.15     # Placeholder (Paper suggests finding via EER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt_orthogonalization(data):\n",
    "    orthogonalized = np.zeros_like(data)\n",
    "    \n",
    "    # 1. First channel (Oz)\n",
    "    v0 = data[0]\n",
    "    orthogonalized[0] = v0\n",
    "    \n",
    "    # 2. Second channel (T7)\n",
    "    if np.dot(v0, v0) == 0: num, den = 0, 1 \n",
    "    else: num, den = np.dot(data[1], v0), np.dot(v0, v0)\n",
    "    v1 = data[1] - (num / den) * v0\n",
    "    orthogonalized[1] = v1\n",
    "    \n",
    "    # 3. Third channel (Cz)\n",
    "    if np.dot(v0, v0) == 0: n1, d1 = 0, 1\n",
    "    else: n1, d1 = np.dot(data[2], v0), np.dot(v0, v0)\n",
    "    \n",
    "    if np.dot(v1, v1) == 0: n2, d2 = 0, 1\n",
    "    else: n2, d2 = np.dot(data[2], v1), np.dot(v1, v1)\n",
    "    \n",
    "    v2 = data[2] - (n1 / d1) * v0 - (n2 / d2) * v1\n",
    "    orthogonalized[2] = v2\n",
    "    \n",
    "    return orthogonalized\n",
    "\n",
    "def preprocess_signal(raw_data):\n",
    "    # Min-Max Normalization\n",
    "    min_vals = np.min(raw_data, axis=1, keepdims=True)\n",
    "    max_vals = np.max(raw_data, axis=1, keepdims=True)\n",
    "    denom = (max_vals - min_vals)\n",
    "    denom[denom == 0] = 1.0\n",
    "    normalized = (raw_data - min_vals) / denom\n",
    "    \n",
    "    return gram_schmidt_orthogonalization(normalized)\n",
    "\n",
    "def create_inputs(raw_data):\n",
    "    n_channels, n_total_samples = raw_data.shape\n",
    "    inputs = []\n",
    "    \n",
    "    start = 0\n",
    "    while start + Config.F_SAMPLING_WINDOW <= n_total_samples:\n",
    "        block = raw_data[:, start : start + Config.F_SAMPLING_WINDOW]\n",
    "        img_segments = []\n",
    "        for i in range(Config.ETA):\n",
    "            seg_start = i * Config.DELTA_STRIDE\n",
    "            seg_end = seg_start + Config.T\n",
    "            segment = block[:, seg_start:seg_end] \n",
    "            img_segments.append(segment.T) \n",
    "            \n",
    "        input_matrix = np.array(img_segments) \n",
    "        inputs.append(input_matrix)\n",
    "        start += Config.AUGMENTATION_STRIDE\n",
    "        \n",
    "    return np.array(inputs)\n",
    "\n",
    "# ==========================================\n",
    "# FIXED LOAD_DATASET FUNCTION\n",
    "# ==========================================\n",
    "def load_dataset(num_subjects=10):\n",
    "    import mne.datasets.eegbci as eegbci\n",
    "    \n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    \n",
    "    print(f\"Loading {num_subjects} subjects...\")\n",
    "    \n",
    "    for subject_id in range(1, num_subjects + 1):\n",
    "        try:\n",
    "            # Load data\n",
    "            path_list = eegbci.load_data(subject_id, [1], path=Config.DATA_PATH, update_path=False)\n",
    "            if not path_list:\n",
    "                print(f\"  Skipping Subject {subject_id}: Download failed.\")\n",
    "                continue\n",
    "            path = path_list[0]\n",
    "            \n",
    "            raw = mne.io.read_raw_edf(path, preload=True, verbose='ERROR')\n",
    "            \n",
    "            # --- FIX 1: STRIP DOTS FROM CHANNEL NAMES ---\n",
    "            # EEGMMIDB channels often come as 'Oz.', 'T7.' etc.\n",
    "            raw.rename_channels(lambda x: x.strip('.'))\n",
    "            \n",
    "            # --- FIX 2: CHECK CHANNELS EXIST ---\n",
    "            available_channels = set(raw.ch_names)\n",
    "            missing = [ch for ch in Config.CHANNELS if ch not in available_channels]\n",
    "            if missing:\n",
    "                print(f\"  Subject {subject_id} missing channels: {missing}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # --- FIX 3: USE MODERN PICK METHOD ---\n",
    "            raw.pick(Config.CHANNELS)\n",
    "            \n",
    "            # Resample\n",
    "            if raw.info['sfreq'] != Config.SFREQ:\n",
    "                raw.resample(Config.SFREQ, verbose='ERROR')\n",
    "                \n",
    "            data = raw.get_data()\n",
    "            processed_data = preprocess_signal(data)\n",
    "            inputs = create_inputs(processed_data)\n",
    "            \n",
    "            if len(inputs) > 0:\n",
    "                X_all.append(inputs)\n",
    "                # Use 0-indexed labels for Sparse Categorical Crossentropy\n",
    "                y_all.append(np.full(len(inputs), subject_id - 1)) \n",
    "                print(f\"  Subject {subject_id}: {len(inputs)} samples loaded.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to load Subject {subject_id}: {e}\")\n",
    "\n",
    "    # --- FIX 4: HANDLE EMPTY DATASET ---\n",
    "    if not X_all: \n",
    "        return None, None\n",
    "    \n",
    "    return np.concatenate(X_all), np.concatenate(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve\n",
    "import mne\n",
    "\n",
    "# Silence TF Warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "class Config:\n",
    "    DATA_PATH = \"eeg_data/\" \n",
    "    CHANNELS = ['Oz', 'T7', 'Cz'] \n",
    "    SFREQ = 160.0\n",
    "    T = 160          \n",
    "    ETA = 20         \n",
    "    DELTA_STRIDE = 4 \n",
    "    F_SAMPLING_WINDOW = 236 \n",
    "    AUGMENTATION_STRIDE = 8 \n",
    "    BATCH_SIZE = 64     \n",
    "    EPOCHS = 30         \n",
    "    LR = 0.0001         \n",
    "    DROPOUT = 0.25      \n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def cosine_distance(v1, v2):\n",
    "    epsilon = 1e-10 # Prevent divide by zero\n",
    "    v1_n = v1 / (np.linalg.norm(v1) + epsilon)\n",
    "    v2_n = v2 / (np.linalg.norm(v2) + epsilon)\n",
    "    return 1.0 - np.dot(v1_n, v2_n)\n",
    "\n",
    "def find_optimal_threshold(gen_scores, imp_scores):\n",
    "    # SAFETY CHECK: Ensure we have both classes\n",
    "    if len(gen_scores) == 0 or len(imp_scores) == 0:\n",
    "        print(\"  [Warning] Not enough data to compute ROC. Using default threshold 0.5\")\n",
    "        return 2.5\n",
    "\n",
    "    y_true = [1] * len(gen_scores) + [0] * len(imp_scores)\n",
    "    y_scores = [-s for s in gen_scores] + [-s for s in imp_scores] \n",
    "    \n",
    "    try:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        fnr = 1 - tpr\n",
    "        eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "        optimal_threshold = -thresholds[eer_index]\n",
    "        \n",
    "        print(f\"  [Auto-Tuning] Found Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"  [Auto-Tuning] Estimated EER: {fpr[eer_index]*100:.2f}%\")\n",
    "        return optimal_threshold\n",
    "    except Exception as e:\n",
    "        print(f\"  [Warning] ROC Calculation failed ({e}). Using default 0.5\")\n",
    "        return 2.5\n",
    "\n",
    "# --- MODEL DEFINITION (Must be included) ---\n",
    "def build_paper_model(num_classes):\n",
    "    inputs = layers.Input(shape=(Config.ETA, Config.T, len(Config.CHANNELS)))\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    fingerprint = layers.Dense(1024, activation='relu', name=\"fingerprint_layer\")(x)\n",
    "    x = layers.Dropout(Config.DROPOUT)(fingerprint)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # REQUESTED SIZES\n",
    "    REQ_TRAIN = 50 \n",
    "    REQ_TEST = 10\n",
    "    \n",
    "    print(\"=== Phase 1: Loading Data ===\")\n",
    "    # Ensure load_dataset is defined (from previous steps) \n",
    "    # If you need the load_dataset function again, let me know. \n",
    "    # Assuming it is already in your notebook memory.\n",
    "    \n",
    "    try:\n",
    "        X_all, y_all = load_dataset(num_subjects=REQ_TRAIN + REQ_TEST)\n",
    "    except NameError:\n",
    "        print(\"CRITICAL: 'load_dataset' function not found. Please run the cell containing 'load_dataset' definition first.\")\n",
    "        exit()\n",
    "\n",
    "    if X_all is None: \n",
    "        print(\"No data loaded. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # 1. Shuffle immediately\n",
    "    X_all, y_all = shuffle(X_all, y_all, random_state=42)\n",
    "\n",
    "    # 2. Check actual number of loaded subjects\n",
    "    unique_subjects = np.unique(y_all)\n",
    "    num_loaded = len(unique_subjects)\n",
    "    print(f\"\\n--- DATA STATUS ---\")\n",
    "    print(f\"Requested: {REQ_TRAIN} Train + {REQ_TEST} Test = {REQ_TRAIN + REQ_TEST}\")\n",
    "    print(f\"Actually Loaded: {num_loaded} Subjects\")\n",
    "\n",
    "    if num_loaded < 2:\n",
    "        print(\"Error: Need at least 2 subjects to run. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # 3. DYNAMIC SPLIT LOGIC (Prevents Crashing)\n",
    "    if num_loaded < (REQ_TRAIN + REQ_TEST):\n",
    "        print(\"Warning: Fewer subjects loaded than requested.\")\n",
    "        # Reserve at least 2 for testing, use rest for training\n",
    "        N_TEST = 2\n",
    "        N_TRAIN = num_loaded - N_TEST\n",
    "        print(f\"Adjusting split to: {N_TRAIN} Train, {N_TEST} Test\")\n",
    "    else:\n",
    "        N_TRAIN = REQ_TRAIN\n",
    "        N_TEST = REQ_TEST\n",
    "\n",
    "    # 4. Apply Split\n",
    "    # We sort unique_subjects to ensure deterministic split\n",
    "    unique_subjects.sort()\n",
    "    train_ids = unique_subjects[:N_TRAIN]\n",
    "    test_ids = unique_subjects[N_TRAIN : N_TRAIN + N_TEST]\n",
    "    \n",
    "    print(f\"Training on Subjects: {train_ids}\")\n",
    "    print(f\"Testing on Subjects: {test_ids}\")\n",
    "\n",
    "    # Create Masks\n",
    "    train_mask = np.isin(y_all, train_ids)\n",
    "    test_mask = np.isin(y_all, test_ids)\n",
    "    \n",
    "    X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "    X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
    "\n",
    "    # Remap training labels to 0..N-1\n",
    "    map_lbl = {old: new for new, old in enumerate(train_ids)}\n",
    "    y_train_map = np.array([map_lbl[y] for y in y_train])\n",
    "\n",
    "    print(f\"\\n=== Phase 2: Training Proxy Classifier ===\")\n",
    "    model = build_paper_model(num_classes=len(train_ids))\n",
    "    model.compile(optimizer=optimizers.RMSprop(learning_rate=Config.LR), \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train_map, epochs=Config.EPOCHS, batch_size=Config.BATCH_SIZE, validation_split=0.1, verbose=1)\n",
    "\n",
    "    print(\"\\n=== Phase 3: Extracting Fingerprinter ===\")\n",
    "    fingerprint_model = models.Model(inputs=model.input, outputs=model.get_layer(\"fingerprint_layer\").output)\n",
    "    \n",
    "    print(\"\\n=== Phase 4: Universal Authentication & Threshold Tuning ===\")\n",
    "    \n",
    "    if len(test_ids) >= 2:\n",
    "        user_a = test_ids[0]\n",
    "        user_b = test_ids[1]\n",
    "        \n",
    "        print(f\"Scenario: Genuine User {user_a} vs Impostor User {user_b}\")\n",
    "        \n",
    "        data_a = X_test[y_test == user_a]\n",
    "        data_b = X_test[y_test == user_b]\n",
    "        \n",
    "        if len(data_a) < 10:\n",
    "            print(f\"Warning: User {user_a} has very little data ({len(data_a)} samples). Results may be unstable.\")\n",
    "\n",
    "        # Split A into Enrollment (50%) and Calibration/Test (50%)\n",
    "        split = len(data_a) // 2\n",
    "        if split == 0: \n",
    "            print(\"Not enough data to split for enrollment. Skipping auth test.\")\n",
    "            exit()\n",
    "            \n",
    "        enroll_a = data_a[:split]\n",
    "        probe_a = data_a[split:]\n",
    "        probe_b = data_b\n",
    "        \n",
    "        # 1. Create Template\n",
    "        enroll_fps = fingerprint_model.predict(enroll_a, verbose=0)\n",
    "        template_a = np.mean(enroll_fps, axis=0)\n",
    "        \n",
    "        # 2. Collect Scores\n",
    "        gen_fps = fingerprint_model.predict(probe_a, verbose=0)\n",
    "        imp_fps = fingerprint_model.predict(probe_b, verbose=0)\n",
    "        \n",
    "        gen_scores = [cosine_distance(template_a, fp) for fp in gen_fps]\n",
    "        imp_scores = [cosine_distance(template_a, fp) for fp in imp_fps]\n",
    "        \n",
    "        # 3. Find Best Threshold\n",
    "        best_threshold = find_optimal_threshold(gen_scores, imp_scores)\n",
    "        \n",
    "        # 4. Apply\n",
    "        accepted_gen = sum(1 for s in gen_scores if s < best_threshold)\n",
    "        rejected_imp = sum(1 for s in imp_scores if s > best_threshold)\n",
    "        \n",
    "        print(f\"\\n--- Final Results (Threshold {best_threshold:.4f}) ---\")\n",
    "        print(f\"Genuine Acceptance Rate (GAR): {accepted_gen/len(gen_scores)*100:.1f}%\")\n",
    "        print(f\"Impostor Rejection Rate (GRR): {rejected_imp/len(imp_scores)*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"Not enough test subjects found for authentication.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
